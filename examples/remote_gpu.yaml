remote:
  host: "gpu-server.example.com"
  username: "ubuntu"
  password: "your-password-here"
  uv:
    path: "~/.benchmark-venv"
    python_version: "3.11"
  dependencies:
    - vllm==0.11.0
    - pyyaml
    - requests
    - huggingface_hub

runs:
  - name: "remote-llama-benchmark"
    engine: "vllm"
    serve:
      model_path: "/data/models/Llama-3.1-8B-Instruct"
      port: 8000
      gpu_memory_utilization: 0.9
      max_model_len: 8192
      max_num_seqs: 256
      dtype: "bfloat16"
      disable_log_requests: true
      enable_expert_parallel: false
      tp_dp_pairs:
        - tp: 1
          dp: 1
          pp: 1
    bench:
      save_results: false
      output_dir: "./benchmark_results"
      context_size: [1024, 2048, 4096]
      concurrency: [50, 100]
      num_prompts: [100]
      output_len: [128]
