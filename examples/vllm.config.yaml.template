runs:
  - name: ""
    engine: "vllm"
    serve:
      model_path: ""
      port: <int>
      gpu_memory_utilization: <float>
      max_model_len: <int>
      max_num_seqs: <int>
      dtype: ""
      disable_log_requests: <bool>
      enable_expert_parallel: <bool>
      tp_dp_pairs:
        - tp: <int>
          dp: <int>
          pp: <int>
        - tp: <int>
          dp: <int>
          pp: <int>
    bench:
      output_dir: ""
      context_size: []
      concurrency: []
      num_prompts: []
      output_len: []
