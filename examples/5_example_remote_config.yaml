remote:
  host: "your-gpu-server.example.com"
  port: 22
  username: "root"
  key_filename: "path/to/your/private/key"
  uv:
    path: ~/.venv
    python_version: "3.11"
  dependencies:
    - pyyaml
    - requests
    - vllm==0.15.0
    - huggingface_hub
    - hf_transfer

benchmark:
  - name: tp2_dp1
    engine: vllm

    model:
      repo_id: "meta-llama/Llama-3.1-8B-Instruct"
      local_dir: "/workspace/models/llama-3.1-8b"
      hf_token: ""  # Use HF_TOKEN env var instead
    
    serve:
      tensor_parallel_size: 2
      max_model_len: 8192
      max_num_seqs: 128
      gpu_memory_utilization: 0.9
      disable_log_requests: true
    
    bench:
      - backend: vllm
        endpoint: /v1/completions
        dataset_name: random
        random_input_len: 512
        random_output_len: 128
        num_prompts: 50
        max_concurrency: 50
        request_rate: inf
        ignore_eos: true
      
      - backend: vllm
        endpoint: /v1/completions
        dataset_name: random
        random_input_len: 1024
        random_output_len: 256
        num_prompts: 50
        max_concurrency: 50
        request_rate: inf
        ignore_eos: true
      
      - backend: vllm
        endpoint: /v1/completions
        dataset_name: random
        random_input_len: 2048
        random_output_len: 512
        num_prompts: 30
        max_concurrency: 30
        request_rate: inf
        ignore_eos: true
    
    results:
      save_result: true
      result_dir: "./benchmark_results"
      save_detailed: true

  - name: tp1_dp1
    engine: vllm

    model:
      local_dir: "/workspace/models/llama-3.1-8b"
    
    serve:
      tensor_parallel_size: 1
      max_model_len: 8192
      max_num_seqs: 128
      gpu_memory_utilization: 0.9
      disable_log_requests: true
    
    bench:
      - backend: vllm
        endpoint: /v1/completions
        dataset_name: random
        random_input_len: 512
        random_output_len: 128
        num_prompts: 50
        max_concurrency: 50
        request_rate: inf
        ignore_eos: true
      
      - backend: vllm
        endpoint: /v1/completions
        dataset_name: random
        random_input_len: 1024
        random_output_len: 256
        num_prompts: 50
        max_concurrency: 50
        request_rate: inf
        ignore_eos: true
      
      - backend: vllm
        endpoint: /v1/completions
        dataset_name: random
        random_input_len: 2048
        random_output_len: 512
        num_prompts: 30
        max_concurrency: 30
        request_rate: inf
        ignore_eos: true
    
    results:
      save_result: true
      result_dir: "./benchmark_results"
      save_detailed: true