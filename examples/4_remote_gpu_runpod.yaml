runpod:
  runpod_api_key: "" # or export RUNPOD_API_KEY
  ssh_private_key: "/path/to/your/private/key"
  pod:
    name: "benchmaxxing-8xh100"
    gpu_type: "NVIDIA A100-SXM4-80GB" # https://docs.runpod.io/references/gpu-types#gpu-types
    gpu_count: 2
    instance_type: on_demand # (spot|on_demand)
    secure_cloud: true
  container:
    image: "runpod/pytorch:2.4.0-py3.11-cuda12.4.1-devel-ubuntu22.04" # docker image
    disk_size: 200 # temporary storage (GB)
  storage:
    volume_size: 200 # persistent storage (GB)
    mount_path: "/workspace"
  ports:
    http: [8888, 8000]
    tcp: [22]
  env:
    HF_HOME: "/workspace/hf_home" # HuggingFace cache directory

# Remote execution config
# When using `benchmaxxing runpod bench`, host/port/username are auto-populated from the pod
# When using `benchmaxxing bench`, you need to provide host/port/username manually
remote:
  # host: ""       # Not needed for e2e - auto-populated from runpod deploy
  # port: 22       # Not needed for e2e - auto-populated from runpod deploy  
  # username: ""   # Not needed for e2e - defaults to "root"
  key_filename: "/path/to/your/private/key"  # Same as runpod.ssh_private_key
  uv:
    path: "~/.benchmark-venv"
    python_version: "3.11"
  dependencies:
    - vllm==0.11.0
    - pyyaml
    - requests
    - huggingface_hub

runs:
  - name: "qwen-3b-benchmark"
    engine: "vllm"

    model:
      repo_id: "Qwen/Qwen2.5-3B-Instruct"
      local_dir: "/workspace/models/qwen-3b" # optional, uses HF_HOME if not set
      hf_token: "" # or export HF_TOKEN

    vllm_serve:
      model_path: "Qwen/Qwen2.5-3B-Instruct"
      port: 8000
      gpu_memory_utilization: 0.9
      max_model_len: 4096
      max_num_seqs: 256
      dtype: "bfloat16"
      disable_log_requests: true

      parallelism_pairs:
        - tensor_parallel: 2
          data_parallel: 1
          pipeline_parallel: 1

    benchmark:
      save_results: true
      output_dir: "/workspace/benchmark_results"
      context_size: [1024, 2048, 3000]
      concurrency: [50, 100]
      num_prompts: [100]
      output_len: [128]
