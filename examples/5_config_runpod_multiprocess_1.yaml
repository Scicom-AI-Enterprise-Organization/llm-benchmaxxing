# Config 1: Single GPU benchmark
runpod:
  ssh_private_key: "/path/to/your/private/key"
  pod:
    name: benchmaq_mp_1xa100pcie_1_unittest
    gpu_type: NVIDIA A100 80GB PCIe
    gpu_count: 1
    instance_type: on_demand
    secure_cloud: true
    deploy_retries: 20
    deploy_retry_interval: 30
  container:
    image: runpod/pytorch:2.4.0-py3.11-cuda12.4.1-devel-ubuntu22.04
    disk_size: 300
  storage:
    volume_size: 300
    mount_path: /workspace
  env:
    HF_HOME: "/workspace/hf_home" # HuggingFace cache directory

remote:
  username: root
  uv:
    path: ~/.benchmark-venv
    python_version: "3.11"
  dependencies:
    - pyyaml
    - requests
    - vllm
    - huggingface_hub

runs:
  - name: run_1
    engine: vllm
    model:
      repo_id: "meta-llama/Llama-3.1-8B-Instruct"
      hf_token: "" # or export HF_TOKEN

    vllm_serve:
      model_path: "meta-llama/Llama-3.1-8B-Instruct"
      port: 8000
      gpu_memory_utilization: 0.9
      disable_log_requests: true
      parallelism_pairs:
        - tensor_parallel: 1
          data_parallel: 1
          pipeline_parallel: 1
    benchmark:
      save_results: true
      output_dir: ./benchmark_results
      context_size: [1024]
      concurrency: [50, 100]
      num_prompts: [100]
      output_len: [128]
      save_results: true
