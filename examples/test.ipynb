{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Parallel Benchmark Test\n",
        "\n",
        "Run multiple benchmarks in parallel on separate RunPod GPU instances.\n",
        "\n",
        "## Features\n",
        "- **Python API**: Configure benchmarks programmatically\n",
        "- **Parallel Execution**: Run multiple pods in parallel using `ThreadPoolExecutor`\n",
        "- **Save Results**: Logs saved to `./benchmark_results/` as `.txt` and `.json`\n",
        "\n",
        "**Note:** Uses `ThreadPoolExecutor` instead of `multiprocessing.Pool` because:\n",
        "- Works in Jupyter notebooks on macOS (no pickling issues)\n",
        "- Benchmark tasks are I/O-bound (API calls), not CPU-bound"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import benchmaxxing\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import os\n",
        "\n",
        "print(f\"benchmaxxing version: {benchmaxxing.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Option 1: Parallel Execution with Config Files\n",
        "\n",
        "Use separate YAML config files for each benchmark configuration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define config files to run in parallel\n",
        "config_files = [\n",
        "    \"6_config_1.yaml\",  # 1x GPU, TP=1\n",
        "    \"6_config_2.yaml\",  # 2x GPU, TP=2\n",
        "    \"6_config_3.yaml\",  # 4x GPU, TP=4 and TP=2+DP=2\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_config_file(config_path: str) -> dict:\n",
        "    \"\"\"Run benchmark from config file.\"\"\"\n",
        "    try:\n",
        "        return {\"config\": config_path, \"result\": benchmaxxing.runpod.bench(config_path)}\n",
        "    except Exception as e:\n",
        "        return {\"config\": config_path, \"status\": \"error\", \"error\": str(e)}\n",
        "\n",
        "\n",
        "# Run all configs in parallel using ThreadPoolExecutor\n",
        "results = []\n",
        "with ThreadPoolExecutor(max_workers=len(config_files)) as executor:\n",
        "    futures = {executor.submit(run_config_file, cfg): cfg for cfg in config_files}\n",
        "    for future in as_completed(futures):\n",
        "        results.append(future.result())\n",
        "\n",
        "for r in results:\n",
        "    print(f\"{r['config']}: {r.get('result', r.get('error'))}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Option 2: Parallel Execution with Python Kwargs\n",
        "\n",
        "Define benchmark configs as Python dicts. Useful when you want:\n",
        "- Dynamic configuration at runtime\n",
        "- Unique `pod_name` per process for identification\n",
        "- Override specific params while sharing a base config file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define multiple benchmark configurations\n",
        "benchmark_configs = [\n",
        "    {\n",
        "        \"config_path\": \"5_python_config.yaml\",  # Base config with model/image settings\n",
        "        \"gpu_count\": 1,\n",
        "        \"pod_name\": \"bench_1gpu\",\n",
        "        \"context_sizes\": [1024, 2048],\n",
        "        \"parallelism_pairs\": [\n",
        "            {\"tensor_parallel\": 1, \"data_parallel\": 1, \"pipeline_parallel\": 1},\n",
        "        ],\n",
        "        \"save_results\": True,\n",
        "    },\n",
        "    {\n",
        "        \"config_path\": \"5_python_config.yaml\",\n",
        "        \"gpu_count\": 2,\n",
        "        \"pod_name\": \"bench_2gpu\",\n",
        "        \"context_sizes\": [1024, 2048, 4096],\n",
        "        \"parallelism_pairs\": [\n",
        "            {\"tensor_parallel\": 2, \"data_parallel\": 1, \"pipeline_parallel\": 1},\n",
        "        ],\n",
        "        \"save_results\": True,\n",
        "    },\n",
        "    {\n",
        "        \"config_path\": \"5_python_config.yaml\",\n",
        "        \"gpu_count\": 4,\n",
        "        \"pod_name\": \"bench_4gpu\",\n",
        "        \"context_sizes\": [1024, 2048, 4096],\n",
        "        \"parallelism_pairs\": [\n",
        "            {\"tensor_parallel\": 4, \"data_parallel\": 1, \"pipeline_parallel\": 1},\n",
        "            {\"tensor_parallel\": 2, \"data_parallel\": 2, \"pipeline_parallel\": 1},\n",
        "        ],\n",
        "        \"save_results\": True,\n",
        "    },\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_benchmark(config: dict) -> dict:\n",
        "    \"\"\"Run benchmark with kwargs.\"\"\"\n",
        "    pod_name = config.get(\"pod_name\", \"unknown\")\n",
        "    try:\n",
        "        return {\"pod_name\": pod_name, \"result\": benchmaxxing.runpod.bench(**config)}\n",
        "    except Exception as e:\n",
        "        return {\"pod_name\": pod_name, \"status\": \"error\", \"error\": str(e)}\n",
        "\n",
        "\n",
        "# Run all benchmarks in parallel (each on separate RunPod pod)\n",
        "results = []\n",
        "with ThreadPoolExecutor(max_workers=len(benchmark_configs)) as executor:\n",
        "    futures = {executor.submit(run_benchmark, cfg): cfg for cfg in benchmark_configs}\n",
        "    for future in as_completed(futures):\n",
        "        results.append(future.result())\n",
        "\n",
        "# Display results\n",
        "for r in results:\n",
        "    print(f\"{r['pod_name']}: {r.get('result', r.get('error'))}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Option 3: Dynamic GPU Scaling Test\n",
        "\n",
        "Generate configs programmatically for GPU scaling tests."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_scaling_config(gpu_count: int) -> dict:\n",
        "    \"\"\"Create config for N GPUs with TP=N.\"\"\"\n",
        "    return {\n",
        "        \"config_path\": \"5_python_config.yaml\",\n",
        "        \"gpu_count\": gpu_count,\n",
        "        \"pod_name\": f\"scale_{gpu_count}gpu\",\n",
        "        \"context_sizes\": [1024, 2048, 4096],\n",
        "        \"concurrency\": [50, 100],\n",
        "        \"num_prompts\": [100],\n",
        "        \"output_len\": [128],\n",
        "        \"parallelism_pairs\": [\n",
        "            {\"tensor_parallel\": gpu_count, \"data_parallel\": 1, \"pipeline_parallel\": 1},\n",
        "        ],\n",
        "        \"save_results\": True,\n",
        "    }\n",
        "\n",
        "\n",
        "# Test scaling from 1 to 8 GPUs\n",
        "gpu_counts = [1, 2, 4, 8]\n",
        "scaling_configs = [create_scaling_config(n) for n in gpu_counts]\n",
        "\n",
        "print(f\"Created {len(scaling_configs)} scaling configs:\")\n",
        "for cfg in scaling_configs:\n",
        "    print(f\"  - {cfg['pod_name']}: {cfg['gpu_count']} GPU(s), TP={cfg['parallelism_pairs'][0]['tensor_parallel']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run scaling test in parallel\n",
        "results = []\n",
        "with ThreadPoolExecutor(max_workers=len(scaling_configs)) as executor:\n",
        "    futures = {executor.submit(run_benchmark, cfg): cfg for cfg in scaling_configs}\n",
        "    for future in as_completed(futures):\n",
        "        results.append(future.result())\n",
        "\n",
        "print(\"=== Scaling Test Results ===\")\n",
        "for r in results:\n",
        "    print(f\"{r['pod_name']}: {r.get('result', r.get('error'))}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Check Saved Results\n",
        "\n",
        "Results are saved to `./benchmark_results/` with naming:\n",
        "```\n",
        "{name}_TP{tp}_DP{dp}_CTX{ctx}_C{concurrency}_P{prompts}_O{output}.txt\n",
        "{name}_TP{tp}_DP{dp}_CTX{ctx}_C{concurrency}_P{prompts}_O{output}.json\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results_dir = \"./benchmark_results\"\n",
        "\n",
        "if os.path.exists(results_dir):\n",
        "    files = sorted(os.listdir(results_dir))\n",
        "    txt_files = [f for f in files if f.endswith(\".txt\")]\n",
        "    json_files = [f for f in files if f.endswith(\".json\")]\n",
        "    \n",
        "    print(f\"Found {len(txt_files)} .txt files and {len(json_files)} .json files:\\n\")\n",
        "    \n",
        "    print(\"TXT files (benchmark logs):\")\n",
        "    for f in txt_files:\n",
        "        print(f\"  {f}\")\n",
        "    \n",
        "    print(\"\\nJSON files (structured results):\")\n",
        "    for f in json_files:\n",
        "        print(f\"  {f}\")\n",
        "else:\n",
        "    print(f\"No results directory found at {results_dir}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Read a sample result file\n",
        "import json\n",
        "\n",
        "if os.path.exists(results_dir):\n",
        "    json_files = [f for f in os.listdir(results_dir) if f.endswith(\".json\")]\n",
        "    if json_files:\n",
        "        sample_file = os.path.join(results_dir, json_files[0])\n",
        "        with open(sample_file) as f:\n",
        "            data = json.load(f)\n",
        "        print(f\"Sample result from {json_files[0]}:\")\n",
        "        print(json.dumps(data, indent=2))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
