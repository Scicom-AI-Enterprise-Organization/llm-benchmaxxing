runs:
  - name: "gpt-oss-120b-run1"
    engine: "vllm"

    model:
      repo_id: "Scicom-intl/gpt-oss-120b-Malaysian-Reasoning-SFT-v0.1"
      local_dir: "/path/to/model" # optional, uses HF_HOME if not set through env

    vllm_serve:
      model_path: "Scicom-intl/gpt-oss-120b-Malaysian-Reasoning-SFT-v0.1" # or "/path/to/model" from model.local_dir
      port: 8000
      gpu_memory_utilization: 0.9
      max_model_len: 12000
      max_num_seqs: 256
      dtype: "bfloat16"
      disable_log_requests: true
      enable_expert_parallel: false

      parallelism_pairs:
        - tensor_parallel: 4 
          data_parallel: 1 
          pipeline_parallel: 1 
          
        - tensor_parallel: 2
          data_parallel: 1
          pipeline_parallel: 1

    benchmark:
      save_results: true
      output_dir: "./benchmark_results"
      context_size: [1024, 2048, 4096, 8192]
      concurrency: [100]
      num_prompts: [100]
      output_len: [128]